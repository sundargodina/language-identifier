# -*- coding: utf-8 -*-
"""language.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15ajEozXu0AEOCGz0igxvx3dL8BpEyf65
"""



"""

After data selection and preprocessing I used 22 languages from the original dataset Which Includes following Languages

⦁ English
⦁ Arabic
⦁ French
⦁ Hindi
⦁ Urdu
⦁ Portuguese
⦁ Persian
⦁ Pushto
⦁ Spanish
⦁ Korean
⦁ Tamil
⦁ Turkish
⦁ Estonian
⦁ Russian
⦁ Romanian
⦁ Chinese
⦁ Swedish
⦁ Latin
⦁ German
⦁ Dutch
⦁ Japanese
⦁ Thai"""

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from keras.models import Sequential
from keras.layers import Dense
import pickle
from sklearn.naive_bayes import MultinomialNB
import seaborn as sns
import matplotlib.pyplot as plt
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import re
nltk.download('stopwords')

data= pd.read_csv("/content/language.csv")

data.Text.head(20),data.language.head(20)

data.shape

data['language'].value_counts()

data['language'][2]

plt.figure(figsize=(20,20))
sns.countplot(data['language'])

ps = PorterStemmer()
corpus=[]

for i in range(len(data['Text'])):

    rev = re.sub("^[a-zA-Z]",' ', data['Text'][i])
    rev = rev.lower()
    rev = rev.split()
    rev = [ps.stem(word) for word in rev if set(stopwords.words())]
    rev = ' '.join(rev)
    corpus.append(rev)

    print(f"{i}")

from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer(max_features=10000)
X = cv.fit_transform(corpus).toarray()

from sklearn.preprocessing import LabelEncoder
label = LabelEncoder()
y = label.fit_transform(data['language'])

label.classes_

y



from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier

from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
classifier = MultinomialNB().fit(X_train,y_train)
ypred = classifier.predict(X_test)

print(accuracy_score(y_test,ypred))
plt.figure(figsize=(20,20))
sns.heatmap(confusion_matrix(y_test,ypred),annot=True,cmap=plt.cm.Accent)

rf=RandomForestClassifier().fit(X_train,y_train)

pred = rf.predict(X_test)

print(accuracy_score(y_test,pred))
plt.figure(figsize=(20,20))
sns.heatmap(confusion_matrix(y_test,pred),annot=True,cmap=plt.cm.Accent)

import pickle
pickle.dump(rf, open('modelrf.pkl', 'wb'))
pickle.dump(classifier, open('modelnb.pkl', 'wb'))

from sklearn.ensemble import VotingClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier

# Create individual classifiers
multinomial_nb_classifier = MultinomialNB()
random_forest_classifier = RandomForestClassifier()

# Create a soft voting classifier
voting_classifier = VotingClassifier(
    estimators=[('multinomial_nb', multinomial_nb_classifier),
                ('random_forest', random_forest_classifier)],
    voting='soft'  # Use soft voting
)

# Train the voting classifier on your data
voting_classifier.fit(X_train, y_train)  # X_train and y_train are your training data

# Make predictions using the ensemble
predictions = voting_classifier.predict(X_test)  # X_test is your test data

print(accuracy_score(y_test,predictions))
plt.figure(figsize=(20,20))
sns.heatmap(confusion_matrix(y_test,predictions),annot=True,cmap=plt.cm.Accent)

pickle.dump(classifier, open('modelensemble.pkl', 'wb'))